<!DOCTYPE HTML>
<!--
	Escape Velocity by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Brain Nexus</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="right-sidebar is-preload">
		<div id="page-wrapper">

			<!-- Header -->
				<section id="header-BCI" class="wrapper">

					<!-- Logo -->
						<div id="logo">
							<h1><a href="Brain-Nexus.html">Brain Nexus</a></h1>
							<p>Converting thoughts into action.</p>
						</div>

					<!-- Nav -->
						<nav id="nav">
							<ul>
								<li class="current"><a href="index.html">Home</a></li>
								<li>
									<a href="#">Projects</a>
									<ul>
										<li>
											<a href="#">Mineral Detection</a>
											<ul>
												<li><a href="Mineral.html">About</a></li>
												<li><a href="https://sites.google.com/view/innovative-mining">Website</a></li>
											</ul>
										</li>
										<li>
											<a href="#">Brain Nexus</a>
											<ul>
												<li><a href="Brain-Nexus.html">About</a></li>
												<li><a href="https://www.instagram.com/brainexus/">Instagram</a></li>
											</ul>
										<li>
										<li>
											<a href="#">Health Mobile</a>
											<ul>
												<li><a href="Health.html">About</a></li>
												<li><a href="https://www.instagram.com/brainexus/">Instagram</a></li>
											</ul>
										</li>
<!--										<li>-->
<!--											<a href="#">Bitbolide</a>-->
<!--											<ul>-->
<!--												<li><a href="Bitbolide.html">About</a></li>-->
<!--												<li><a href="https://www.linkedin.com/company/bitbolide/?originalSubdomain=ca">LinkedIn</a></li>-->
<!--											</ul>-->
<!--										</li>-->
									</ul>
								</li>
								<li><a href="Images.html">Images</a></li>
								<li><a href="Videos.html">Videos</a></li>
							</ul>
						</nav>

				</section>

			<!-- Main -->
				<section id="main" class="wrapper style1">
					<div class="title">Description</div>
					<div class="container">
						<div class="row gtr-150">
							<div class="col-8 col-12-medium">

								<!-- Content -->
									<div id="content">
										<article class="box post">
											<header class="style1">
												<h2>Brain-Computer Interface for movement intention prediction</h2>
<!--												<p>Tempus feugiat veroeros sed nullam dolore</p>-->
											</header>
											<a href="#" class="image featured">
												<img src="images/headset.npg" alt="" />
											</a>
											<p>This work has resulted from the joint effort of a multidisciplinary group of specialists
												in Artificial Intelligence, mechanics, and medicine. The result is a Brain-Computer
												Interface (BCI) that allows controlling video games through electroencephalography
												(EEG) signals generated by human thoughts and acquired through a headset equipped with EEG electrodes.
												The generated signals are produced when the user executes imaginary legs, arms, and tongue movements.
												Machine learning models classify those signals that translate into commands to control multiple video games.
												This tool has potential applications in the treatment and rehabilitation of stroke
												patients for the recovery of lost motor and cognitive abilities.</p>
											<p>Our tool consists of a headset with seven electrodes connected to a circuit that digitalizes the electrical
												signals they capture, converting them into time series. The final model must be able to classify five limb
												movements imagined by the user. The project's first stage consists of collecting data to train the model.
												This is done following a pre-established protocol in which the user is instructed to imagine one of these
												movements randomly selected every three seconds using cues that can be visual or audio. Figure 1
												 shows examples of cues presentation for data collection, and Figure 2 shows the headset we constructed
												with adjustable electrode positions. </p>
											<div class="row gtr-150">
												<div class="col-6 col-12-small">
													<section class="box">
														<a class="image featured"><img src="images/Ques.png" alt="" /></a>
														<p style="font-size: 12px;"> Fig. 1: <em> Every time a cue is highlighted, the user should imagine a movement with the specific limb shown
															In this specific screenshot, the user is asked to imagine a movement with the left hand (or arm).</em></p>
													</section>
												</div>
												<div class="col-6 col-12-small">
													<section class="box">
														<a class="image featured"><img src="images/headset.png" alt="" /></a>
														<p style="font-size: 12px;"> Fig. 2: <em> Final design of the headset with adjustable electrode positions (X, Y, Z). </em></p>
													</section>
												</div>
											</div>
											<p></p>
											<p>We then performed an experiment to determine which cues generated the best identifiable electrical signals.
												To do this, we used a simple linear model coupled with a feature engineering process to construct
												Fast Fourier Transform (FFT) features for the collected time series.
											</p>

											<section class="box">
												<a class="image featured"><img src="images/Cue_compare.png" alt="" style="max-width: 70%; height: auto;"/></a>
												<p style="font-size: 12px;"> Fig. 3: <em> Using Logistic Regression and FFT-features
												to determine the best cue-type for data collection</em></p>
											</section>
											<p>The same model is used to determine the best electrode placement in a manual electrode
												placement experiment directly on the skull,
												as shown in the following figure.</p>
											<section class="box">
												<a class="image featured"><img src="images/electrodes.png" alt="" /></a>
												<p style="font-size: 12px;"> Fig. 4: <em> One cue is shown at a time without showing the others.
															In this specific screenshot, the user is asked to imagine a movement with the left hand (or arm). </em></p>
											</section>
											<p>Several preprocessing operations, such as bias correction and noise reduction, are then performed.
												The final classification model consists of a recurrent neural network with a horizon of 25 samples equivalent to
												10 milliseconds since our sampling frequency is 250 samples per second. The final classification results are shown in the following confusion matrix.</p>
											<div class="row gtr-150">
											<div class="col-6 col-12-small">
												<section class="box">
													<a class="image featured"style="display: block; text-align: center;">
														<img src="images/confusion.png" alt="" style="max-width: 90%; height: auto;" /></a>
													<p style="font-size: 12px;"> Fig. 5: <em> Confusion matrix showing the number of
														correct and incorrect predictions for classes left arm, right arm, left leg, right leg and rest</em></p>
												</section>
											</div>
											</div>

										</article>
									</div>
							</div>

							<div class="col-4 col-12-medium">

								<!-- Sidebar -->
									<div id="sidebar">
										<section class="box">
											<header>
												<h2>Some result in images</h2>
											</header>
											<p> Here we show in images some project partial results at different stages.</p>
											<section class="box">
												<a class="image featured"style="display: block; text-align: center;">
													<img src="images/electrode_performance.png" alt="" style="max-width: 90%; height: auto;" /></a>
												<p style="font-size: 12px;">  <em> Impedance changes electrodes exhibit between
													different recording sessions and during the same Session. We can see as it goes deteriorating with time
												so new electrodes are always desirable for starting a new session.</em></p>
											</section>
											<section class="box">
												<a class="image featured"style="display: block; text-align: center;">
													<img src="images/cleaning_signal.png" alt="" style="max-width: 110%; height: auto;" /></a>
												<p style="font-size: 12px;">  <em> Bias correction, low-frequency noise reduction and muscular movement detection.
													The red continuous line determine is the detected byas and low noise frequencies to remove.
													The doted red line is a threshold above which we considered the recording to contain an interference
													which could be due to muscular movement or electrical testabilities</em></p>
											</section>

											<section class="box">
												<a class="image featured"style="display: block; text-align: center;">
													<img src="images/Cues_Compare_by_Electrode.png" alt="" style="max-width: 100%; height: auto;" /></a>
												<p style="font-size: 12px;">  <em> EEG patterns of Left-Hand imaginary movements obtained by adding all recordings
													for each of the seven electrodes (Fz, C3, Cz, C4, P3, Pz, and P4). Black, Red, and Blue lines correspond to
													all images on the screen, center images, and audio cues, respectively.</em></p>
											</section>

											<section class="box">
												<a class="image featured"style="display: block; text-align: center;">
													<img src="images/Game_Launcher.png" alt="" style="max-width: 70%; height: auto;" /></a>
												<p style="font-size: 12px;">  <em> The game launcher allows the user to select a game from Tetris,
													Ping Pong, Maze, Simon Says, and Snake. It also has a calibration module to fine tune the
													model to the current user. The Game Launcher is controled using the model and the same imaginary
													movements the model learned to classify </em></p>
											</section>

										</section>
									</div>
							</div>
						</div>
					</div>
				</section>
		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.dropotron.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>